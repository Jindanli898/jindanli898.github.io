<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" 
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="jemdoc.css" type="text/css" />
  <link rel="stylesheet" href="custom.css" type="text/css" />
  <title>Jindan Li</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
  <div class="menu-category">MENU</div>
  <div class="menu-item"><a href="index.html" class="current">Home</a></div>
  <div class="menu-item"><a href="award.html">Award</a></div>
</td>

<td id="layout-content">
<div id="toptitle">
<h1>Jindan Li</h1>

<table class="imgtable"><tr><td>
  <a href="https://github.com/Jindanli898" target="_blank">
    <img src="jindanli.jpg" alt="Jindan Li"
         style="width:190px; height:190px; object-fit:cover; border-radius:16px;">
  </a>&nbsp;
</td>

<td align="left">
  <p>
    Ph.D. student<br />
    <a href="https://www.ece.cornell.edu/ece" target="_blank">Department of Electrical and Computer Engineering</a><br />
    <a href="https://tech.cornell.edu/" target="_blank">Cornell Tech</a>
  </p>
  <p>
    Email: jl4767@cornell.edu
  </p>

  <p>
    2 W Loop Rd, New York, NY 10044
  </p>
</td></tr></table>


<h2>About me</h2>
<p>
I am currently a Ph.D. student at Cornell University (since Aug 15, 2025), and am fortunately advised by <a href="https://chen.tech.cornell.edu/" target="_blank">Prof. Tianyi Chen</a>. From Sep 2024 to Aug 2025, I worked as a Research Assistant at RPI ECSE with 
Prof. Chen. In Spring 2025, I also served as a Teaching Assistant for the undergraduate course <i>ECSE 2500 – Engineering Probability</i> at RPI.
I received my B.Eng. in Information Engineering from Zhejiang University in July 2024.
</p>

<!-- <h2>Research</h2>
<p><b>Research interests:</b> Analog In-Memory Computing (AIMC), wireless communication.</p>

<p>
<b>Residual Learning for AIMC Training.</b>  
My recent work focuses on a residual learning strategy across multiple analog tiles to overcome the challenges of limited conductance states and asymmetric update dynamics in AIMC. 
The key idea is to decompose weight updates into residual components with geometric scaling, ensuring each tile operates within a stable dynamic range. 
This approach improves effective precision and robustness under device non-idealities.  

</p> -->
<h2>Research</h2>

<p><b>Research interests:</b> Analog In-Memory Computing (AIMC), in-memory (in-situ) training algorithms, and wireless communication.</p>

<p>
My research focuses on algorithmic training frameworks that operate natively on analog hardware. The goal is to make in-memory training robust to limited precision and device asymmetry while preserving the parallelism and efficiency advantages of AIMC.
</p>
  
<p>
Modern deep learning training increasingly suffers from the cost of moving data between memory and compute units, which limits both throughput and energy efficiency in conventional von Neumann architectures. Analog In-Memory Computing (AIMC) offers a promising alternative by performing computation directly inside memory arrays. In AIMC accelerators, neural network parameters are stored as the conductance states of resistive devices arranged in crossbar arrays, and matrix–vector multiplication (MVM) can be executed in a highly parallel and energy-efficient manner by leveraging circuit physics (e.g., Ohm’s and Kirchhoff’s laws).
</p>

<p>
To fully harvest the efficiency of AIMC, it is crucial to enable in-memory training , where weight updates are applied directly on analog arrays via rank-update mechanisms driven by pulse streams. However, accurate training on real analog hardware is challenging due to intrinsic non-idealities. In particular, many practical memristive devices provide only a limited number of stable conductance states (often around 4-bit resolution), and their update behavior is asymmetric  and state-dependent. These effects lead to unstable convergence and degraded accuracy.
</p>



<p>
<b>Multi-tile Residual Learning for In-Memory Analog Training.</b>
My recent work proposes a multi-tile residual learning strategy to overcome the limited conductance-state bottleneck in AIMC training. The key idea is to represent each weight as a composite sum across multiple analog tiles, where each tile stores a residual component with geometric scaling. This structure expands the effective representable precision beyond what a single low-state device can provide. During training, tiles are coordinated using a multi-timescale schedule: earlier tiles capture coarse updates, and subsequent tiles progressively learn the residual left by preceding tiles, refining the solution over time. This residual decomposition improves robustness under device non-idealities (e.g., discretized updates and asymmetric responses) and supports stable convergence in low-state regimes.
I validate the approach on standard image classification benchmarks (e.g., MNIST/Fashion-MNIST and CIFAR-10/100 with ResNet-style models) using analog training simulator, and analyze the associated efficiency trade-offs when scaling the number of tiles.
</p>

<h2>News</h2>
<p>
  Our paper has been accepted to <b>AISTATS 2026</b>. The full paper is available on
  <a href="https://arxiv.org/pdf/2510.02516" target="_blank" rel="noopener noreferrer">arXiv</a>, and the
  implementation and experimental results are available in my code repository:
  <a href="https://github.com/Jindanli898/AIMC" target="_blank" rel="noopener noreferrer">github.com/Jindanli898/AIMC</a>.
</p>


  
<h2>Education</h2>
<ul>
  <li><a href="https://www.cornell.edu/" target="_blank">Cornell Tech University</a>, New York, NY, USA — Ph.D., Electrical and Computer Engineering, <b>Aug 2025 – Present</b></li>
  <li><a href="https://www.rpi.edu" target="_blank">Rensselaer Polytechnic Institute</a>, Troy, NY, USA — Research Assistant (RA), <b>Sep 2024 – Aug 2025</b></li>
  <li><a href="https://www.zju.edu.cn/" target="_blank">Zhejiang University</a>, Hangzhou, China — B.Eng., Information Engineering, <b>2020 – 2024</b></li>
</ul>

</td>
</tr>
</table>
</body>
</html>
